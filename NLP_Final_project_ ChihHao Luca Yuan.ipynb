{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project\n",
    "ChihHao Luca Yuan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import sentence_polarity\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.tsv\", sep='\\t', index_col=\"PhraseId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"test.tsv\", sep='\\t', index_col=\"PhraseId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(\"sampleSubmission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PhraseId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>of escapades demonstrating the adage that what...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>of</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>escapades demonstrating the adage that what is...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>escapades</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>demonstrating the adage that what is good for ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          SentenceId                                             Phrase  \\\n",
       "PhraseId                                                                  \n",
       "1                  1  A series of escapades demonstrating the adage ...   \n",
       "2                  1  A series of escapades demonstrating the adage ...   \n",
       "3                  1                                           A series   \n",
       "4                  1                                                  A   \n",
       "5                  1                                             series   \n",
       "6                  1  of escapades demonstrating the adage that what...   \n",
       "7                  1                                                 of   \n",
       "8                  1  escapades demonstrating the adage that what is...   \n",
       "9                  1                                          escapades   \n",
       "10                 1  demonstrating the adage that what is good for ...   \n",
       "\n",
       "          Sentiment  \n",
       "PhraseId             \n",
       "1                 1  \n",
       "2                 2  \n",
       "3                 2  \n",
       "4                 2  \n",
       "5                 2  \n",
       "6                 2  \n",
       "7                 2  \n",
       "8                 2  \n",
       "9                 2  \n",
       "10                2  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrasedata = train[['Phrase', 'Sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PhraseId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156056</th>\n",
       "      <td>Hearst 's</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156057</th>\n",
       "      <td>forced avuncular chortles</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156058</th>\n",
       "      <td>avuncular chortles</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156059</th>\n",
       "      <td>avuncular</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156060</th>\n",
       "      <td>chortles</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>156060 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     Phrase  Sentiment\n",
       "PhraseId                                                              \n",
       "1         A series of escapades demonstrating the adage ...          1\n",
       "2         A series of escapades demonstrating the adage ...          2\n",
       "3                                                  A series          2\n",
       "4                                                         A          2\n",
       "5                                                    series          2\n",
       "...                                                     ...        ...\n",
       "156056                                            Hearst 's          2\n",
       "156057                            forced avuncular chortles          1\n",
       "156058                                   avuncular chortles          3\n",
       "156059                                            avuncular          2\n",
       "156060                                             chortles          2\n",
       "\n",
       "[156060 rows x 2 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrasedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(phrasedata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrasedata = phrasedata.values.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(phrasedata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = int(20000)\n",
    "random.shuffle(phrasedata)\n",
    "phraselist = phrasedata[:limit]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 156060 phrases, using 20000 random phrases\n"
     ]
    }
   ],
   "source": [
    "print('Read', len(phrasedata), 'phrases, using', len(phraselist), 'random phrases')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['that movie', 2]\n",
      "['a mediocre collection', 2]\n",
      "['movie types', 2]\n",
      "['Christmas !', 2]\n",
      "['was produced by Jerry Bruckheimer and directed by Joel Schumacher , and reflects the worst of their shallow styles :', 1]\n",
      "['especially charm', 3]\n",
      "['mind images of a violent battlefield action picture', 2]\n",
      "[\"The movie 's gloomy atmosphere is fascinating , though , even if the movie itself does n't stand a ghost of a chance .\", 2]\n",
      "['a window', 2]\n",
      "['has less spice', 2]\n"
     ]
    }
   ],
   "source": [
    "for phrase in phraselist[:10]:\n",
    "    print (phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # create list of phrase documents as (list of words, label)\n",
    "  phrasedocs = []\n",
    "  # add all the phrases\n",
    "  for phrase in phraselist:\n",
    "    tokens = nltk.word_tokenize(phrase[0])\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    phrasedocs.append((tokens, int(phrase[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_rows = len(phrasedocs)\n",
    "total_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', ',', 'a', 'of', 'and', 'to', '.', \"'s\", 'in', 'is', 'that', 'it', 'as', 'with', 'for', 'its', 'an', 'film', 'movie', 'this', 'but', 'on', 'you', 'be', 'more', 'his', 'by', '``', \"n't\", '--', 'one', 'from', 'about', 'at', 'than', 'not', 'or', 'have', 'like', 'all', 'are', \"'\", 'has', '-rrb-', 'story', 'so', 'out', 'who', 'into', '-lrb-']\n"
     ]
    }
   ],
   "source": [
    "all_words_list = [word for (sent,sentiment) in phrasedocs for word in sent]\n",
    "all_words = nltk.FreqDist(all_words_list)\n",
    "\n",
    "word_items = all_words.most_common(2000)\n",
    "word_features = [word for (word, freq) in word_items]\n",
    "print(word_features[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_features(document, word_features):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['V_{}'.format(word)] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets = [(document_features(d,word_features), c) for (d,c) in phrasedocs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = train_test_split(featuresets, test_size=0.2, random_state=42)   #42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy : 0.56725 - 2000 most frequency words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.56725\n"
     ]
    }
   ],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print (nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross Valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = [c for (d,c) in phrasedocs_2]\n",
    "labels = list(set(label_list))    # gets only unique labels\n",
    "num_folds = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each fold size: 6666\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "\n",
      "Average Precision\tRecall\t\tF1 \tPer Label\n",
      "0 \t      0.243      0.210      0.225\n",
      "1 \t      0.277      0.418      0.333\n",
      "2 \t      0.827      0.637      0.720\n",
      "3 \t      0.260      0.429      0.324\n",
      "4 \t      0.229      0.315      0.264\n",
      "\n",
      "Macro Average Precision\tRecall\t\tF1 \tOver All Labels\n",
      "\t      0.367      0.402      0.373\n",
      "\n",
      "Label Counts {0: 897, 1: 3503, 2: 10173, 3: 4223, 4: 1204}\n",
      "Micro Average Precision\tRecall\t\tF1 \tOver All Labels\n",
      "\t      0.549      0.516      0.519\n"
     ]
    }
   ],
   "source": [
    "label_list = [c for (d,c) in phrasedocs]\n",
    "labels = list(set(label_list))    # gets only unique labels\n",
    "num_folds = 3\n",
    "cross_validation_PRF(num_folds, featuresets, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Stop words -> Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "negationwords = ['no', 'not', 'never', 'none', 'nowhere', 'nothing',\n",
    "                 'noone', 'rather', 'hardly', 'scarcely', 'rarely', 'seldom', 'neither', 'nor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "newstopwords = [word for word in stopwords if word not in negationwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "176"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(newstopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newstopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words_list_wo_stopwords = [word for word in all_words_list if word not in newstopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_all_words = nltk.FreqDist(all_words_list_wo_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_word_items = new_all_words.most_common(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_word_features = [word for (word, count) in new_word_items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets_wo_stopwords = [(document_features(d,new_word_features), c) for (d,c) in phrasedocs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wo_stopwords_train_set, wo_stopwords_test_set = train_test_split(featuresets_wo_stopwords, test_size=0.2, random_state=42)   #42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy : 0.575"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.575\n"
     ]
    }
   ],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(wo_stopwords_train_set)\n",
    "print (nltk.classify.accuracy(classifier, wo_stopwords_test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each fold size: 6666\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "\n",
      "Average Precision\tRecall\t\tF1 \tPer Label\n",
      "0 \t      0.183      0.246      0.210\n",
      "1 \t      0.273      0.426      0.333\n",
      "2 \t      0.843      0.632      0.723\n",
      "3 \t      0.286      0.438      0.346\n",
      "4 \t      0.199      0.315      0.244\n",
      "\n",
      "Macro Average Precision\tRecall\t\tF1 \tOver All Labels\n",
      "\t      0.357      0.411      0.371\n",
      "\n",
      "Label Counts {0: 897, 1: 3503, 2: 10173, 3: 4223, 4: 1204}\n",
      "Micro Average Precision\tRecall\t\tF1 \tOver All Labels\n",
      "\t      0.557      0.519      0.523\n"
     ]
    }
   ],
   "source": [
    "cross_validation_PRF(num_folds, featuresets_wo_stopwords, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lemma process -> stopwords -> BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "phrasedocs_2 = []\n",
    "for phrase in phraselist:\n",
    "    tokens = nltk.word_tokenize(phrase[0])  \n",
    "    tokens = [token.lower() for token in tokens]  \n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]  # Lemmatization\n",
    "    phrasedocs_2.append((lemmatized_tokens, int(phrase[1])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets_wo_stopwords_lemma = [(document_features(d,new_word_features), c) for (d,c) in phrasedocs_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "wo_stopwords_train_set_lemma, wo_stopwords_test_set_lemma = train_test_split(featuresets_wo_stopwords_lemma, test_size=0.2, random_state=42)   #42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.57575"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.57575\n"
     ]
    }
   ],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(wo_stopwords_train_set_lemma)\n",
    "print (nltk.classify.accuracy(classifier, wo_stopwords_test_set_lemma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Stop Words then Negation words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NOT_features(document, word_features, negationwords):\n",
    "    features = {}\n",
    "    \n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = False\n",
    "        features['contains(NOT{})'.format(word)] = False\n",
    "    # go through document words in order\n",
    "    for i in range(0, len(document)):\n",
    "        word = document[i]\n",
    "        if ((i + 1) < len(document)) and ((word in negationwords) or (word.endswith(\"n't\"))):\n",
    "            i += 1\n",
    "            features['contains(NOT{})'.format(document[i])] = (document[i] in word_features)\n",
    "        else:\n",
    "            features['contains({})'.format(word)] = (word in word_features)\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOT_featuresets = [(NOT_features(d, new_word_features, negationwords), c) for (d, c) in phrasedocs_2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_NOT_Feature, test_set_NOT_Feature = train_test_split(NOT_featuresets, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove stop words + negation, Accuracy = 0.51525 (freq 2000 words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.51525\n"
     ]
    }
   ],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(train_set_NOT_Feature)\n",
    "print (nltk.classify.accuracy(classifier, test_set_NOT_Feature))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readSubjectivity(path):\n",
    "\tflexicon = open(path, 'r')\n",
    "\t# initialize an empty dictionary\n",
    "\tsldict = { }\n",
    "\tfor line in flexicon:\n",
    "\t\tfields = line.split()   # default is to split on whitespace\n",
    "\t\t# split each field on the '=' and keep the second part as the value\n",
    "\t\tstrength = fields[0].split(\"=\")[1]\n",
    "\t\tword = fields[2].split(\"=\")[1]\n",
    "\t\tposTag = fields[3].split(\"=\")[1]\n",
    "\t\tstemmed = fields[4].split(\"=\")[1]\n",
    "\t\tpolarity = fields[5].split(\"=\")[1]\n",
    "\t\tif (stemmed == 'y'):\n",
    "\t\t\tisStemmed = True\n",
    "\t\telse:\n",
    "\t\t\tisStemmed = False\n",
    "\t\t# put a dictionary entry with the word as the keyword\n",
    "\t\t#     and a list of the other values\n",
    "\t\tsldict[word] = [strength, posTag, isStemmed, polarity]\n",
    "\treturn sldict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "SLpath = \"/Users/simba/Desktop/semester 3/NLP/Final Project/FinalProjectData (5)/FinalProjectData/kagglemoviereviews/corpus/SentimentLexicons/subjclueslen1-HLTEMNLP05.tff\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "SL = readSubjectivity(SLpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SL_features(document, SL):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    \n",
    "    for word in new_word_features:\n",
    "        features['contains(%s)' % word] = (word in document_words)\n",
    "        \n",
    "    # count variables for the 4 classes of subjectivity\n",
    "    weakPos = 0\n",
    "    strongPos = 0\n",
    "    weakNeg = 0\n",
    "    strongNeg = 0\n",
    "    \n",
    "    for word in document_words:\n",
    "        if word in SL:\n",
    "            strength, posTag, isStemmed, polarity = SL[word]\n",
    "            if strength == 'weaksubj' and polarity == 'positive':\n",
    "                weakPos += 1\n",
    "            if strength == 'strongsubj' and polarity == 'positive':\n",
    "                strongPos += 1\n",
    "            if strength == 'weaksubj' and polarity == 'negative':\n",
    "                weakNeg += 1\n",
    "            if strength == 'strongsubj' and polarity == 'negative':\n",
    "                strongNeg += 1\n",
    "                \n",
    "            features['positivecount'] = weakPos + (2 * strongPos)\n",
    "            features['negativecount'] = weakNeg + (2 * strongNeg)\n",
    "    \n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "SL_featuresets = [(SL_features(d, SL), c) for (d,c) in phrasedocs_2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SL : 0.576"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.576\n"
     ]
    }
   ],
   "source": [
    "train_set_SL, test_set_SL = train_test_split(SL_featuresets, test_size=0.2, random_state=42)   #42\n",
    "\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set_SL)\n",
    "print(nltk.classify.accuracy(classifier, test_set_SL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SL_features(document, SL):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    \n",
    "    for word in new_word_features:\n",
    "        features['contains(%s)' % word] = (word in document_words)\n",
    "        \n",
    "    # count variables for the 4 classes of subjectivity\n",
    "    weakPos = 0\n",
    "    strongPos = 0\n",
    "    weakNeg = 0\n",
    "    strongNeg = 0\n",
    "    \n",
    "    for word in document_words:\n",
    "        if word in SL:\n",
    "            strength, posTag, isStemmed, polarity = SL[word]\n",
    "            if strength == 'weaksubj' and polarity == 'positive':\n",
    "                weakPos += 1\n",
    "            if strength == 'strongsubj' and polarity == 'positive':\n",
    "                strongPos += 1\n",
    "            if strength == 'weaksubj' and polarity == 'negative':\n",
    "                weakNeg += 1\n",
    "            if strength == 'strongsubj' and polarity == 'negative':\n",
    "                strongNeg += 1\n",
    "                \n",
    "            features['positivecount'] = weakPos + (2 * strongPos)\n",
    "            features['negativecount'] = weakNeg + (2 * strongNeg)\n",
    "    \n",
    "    return features\n",
    "\n",
    "SL_featuresets = [(SL_features(d, SL), c) for (d,c) in phrasedocs_2]\n",
    "\n",
    "train_set_SL, test_set_SL = train_test_split(SL_featuresets, test_size=0.2, random_state=42)   #42\n",
    "\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set_SL)\n",
    "print(nltk.classify.accuracy(classifier, test_set_SL))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bi Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "finder = BigramCollocationFinder.from_words(all_words_list_wo_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(\"'30s\", \"'40s\"), ('10-course', 'banquet'), ('18-year-old', 'mistress'), ('1915', 'armenia'), ('1959', 'godzilla'), ('1962', 'garnish'), ('20,000', 'leagues'), ('20-car', 'pileup'), ('20th-century', 'footnotes'), ('22-year-old', 'girlfriend'), ('9\\\\/11', 'juxtaposition'), ('\\\\', '*'), ('abbott', 'ernest'), ('accomplishment', 'veracity'), ('ace', 'japanimator'), ('achieved', 'lottery'), ('action-and-popcorn', 'obsessed'), ('action\\\\/thriller', 'well-balanced'), ('actorish', 'notations'), ('actory', 'concoctions'), ('adopts', 'guise'), ('adorns', 'family-film'), ('adrien', 'brody'), ('advanced', 'prozac'), ('affirms', 'nourishing'), ('after-hours', 'loopiness'), ('agency', 'boss'), ('agnostic', 'carnivore'), ('aisles', 'bathroom'), ('alert', 'street-smart'), ('all-night', 'tequila'), ('all-star', 'reunions'), ('ally', 'mcbeal-style'), ('american-russian', 'armageddon'), ('amidst', 'swirl'), ('ancillary', 'products'), ('angela', 'gheorghiu'), ('animations', 'constricted'), ('anna', 'mouglalis'), ('annoyance', 'chatty'), ('anonymous', 'attackers'), ('anxieties', 'sidesteps'), ('appeared', '1938'), ('applying', 'smear'), ('arbitrary', 'sun-drenched'), ('aristocrat', 'tattered'), ('arnie', 'musclefest'), ('ash', 'wednesday'), ('assets', 'commend'), ('astronauts', 'floating')]\n"
     ]
    }
   ],
   "source": [
    "bigram_features = finder.nbest(bigram_measures.chi_sq, 500)\n",
    "print(bigram_features[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_document_features(document, word_features, bigram_features):\n",
    "    document_words = set(document)\n",
    "    document_bigrams = nltk.bigrams(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['V_{}'.format(word)] = (word in document_words)\n",
    "    for bigram in bigram_features:\n",
    "        features['B_{}_{}'.format(bigram[0], bigram[1])] = (bigram in document_bigrams)    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_featuresets = [(bigram_document_features(d, new_word_features, bigram_features), c) for (d, c) in phrasedocs_2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BIgram 0.57575 with stop words remove\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.57575"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set_bi, test_set_bi = train_test_split(bigram_featuresets, test_size=0.2, random_state=42)   #42\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set_bi)\n",
    "nltk.classify.accuracy(classifier, test_set_bi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS TAG FIRST then BiGram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tags_all_words_list = pos_tag(all_words_list_wo_stopwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "finder_POS = BigramCollocationFinder.from_words(pos_tags_all_words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((\"'30s\", 'POS'), (\"'40s\", 'CD')), ((\"'50s\", 'CC'), ('sociology', 'NN')), ((\"'em\", 'POS'), ('powerment', 'NN')), (('-lrb-', 'FW'), ('goldbacher', 'FW')), (('-rrb-', 'CD'), ('civilized', 'JJ')), (('-rrb-', 'RP'), ('later', 'JJ')), (('10-course', 'JJ'), ('banquet', 'NN')), (('102-minute', 'JJ'), ('infomercial', 'NN')), (('18-year-old', 'JJ'), ('mistress', 'NN')), (('1915', 'CD'), ('armenia', 'NN')), (('1959', 'CD'), ('godzilla', 'NN')), (('1962', 'CD'), ('garnish', 'NN')), (('20,000', 'CD'), ('leagues', 'NNS')), (('20-car', 'JJ'), ('pileup', 'NN')), (('20th-century', 'JJ'), ('footnotes', 'NNS')), (('22-year-old', 'JJ'), ('girlfriend', 'NN')), (('85-minute', 'CD'), ('screwball', 'NN')), (('9\\\\/11', 'CD'), ('juxtaposition', 'NN')), (('\\\\', 'CC'), ('*', 'JJ')), (('abbott', 'NN'), ('ernest', 'JJS')), (('abc', 'VBP'), ('africa', 'JJ')), (('abysmal', 'JJ'), ('hannibal', 'NN')), (('acceptable', 'VB'), ('printed', 'JJ')), (('accomplishment', 'JJ'), ('veracity', 'NN')), (('accumulate', 'NN'), ('veil', 'NN')), (('ace', 'NN'), ('japanimator', 'NN')), (('achero', 'NN'), ('manas', 'NN')), (('achieved', 'VBN'), ('lottery', 'NN')), (('acidic', 'JJ'), ('all-male', 'JJ')), (('action-and-popcorn', 'JJ'), ('obsessed', 'JJ')), (('action\\\\/thriller', 'NN'), ('well-balanced', 'JJ')), (('actorish', 'JJ'), ('notations', 'NNS')), (('actory', 'JJ'), ('concoctions', 'NNS')), (('add', 'NN'), ('canon', 'NN')), (('added', 'VBN'), ('clout', 'NN')), (('adopts', 'VBZ'), ('guise', 'NN')), (('adorns', 'VBZ'), ('family-film', 'JJ')), (('adrien', 'JJ'), ('brody', 'NN')), (('advanced', 'JJ'), ('prozac', 'NN')), (('advanced', 'VBD'), ('prozac', 'JJ')), (('advances', 'NNS'), ('unsurprising', 'VBG')), (('advances', 'VBZ'), ('daringly', 'RB')), (('advice', 'RB'), ('not-so-divine', 'JJ')), (('affinity', 'NN'), ('slapstick', 'RB')), (('affirms', 'NNS'), ('nourishing', 'VBG')), (('afraid', 'VBD'), ('provoke', 'JJ')), (('after-hours', 'JJ'), ('loopiness', 'NN')), (('afterlife', 'NN'), ('communications', 'NNS')), (('afterthought', 'JJ'), ('amidst', 'NN')), (('aged', 'VBD'), ('napoleon', 'NN'))]\n"
     ]
    }
   ],
   "source": [
    "bigram_features_POS = finder_POS.nbest(bigram_measures.chi_sq, 500)\n",
    "print(bigram_features_POS[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_featuresets_POS = [(bigram_document_features(d, new_word_features, bigram_features_POS), c) for (d, c) in phrasedocs]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.56225"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.56225"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set_bi_POS, test_set_bi_POS = train_test_split(bigram_featuresets_POS, test_size=0.2, random_state=42)   #42\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set_bi_POS)\n",
    "nltk.classify.accuracy(classifier, test_set_bi_POS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS TAG FEATURES WITH Bi-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def POS_features(document, word_features):\n",
    "    document_words = set(document)\n",
    "    tagged_words = nltk.pos_tag(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    numNoun = 0\n",
    "    numVerb = 0\n",
    "    numAdj = 0\n",
    "    numAdverb = 0\n",
    "    for (word, tag) in tagged_words:\n",
    "        if tag.startswith('N'): numNoun += 1\n",
    "        if tag.startswith('V'): numVerb += 1\n",
    "        if tag.startswith('J'): numAdj += 1\n",
    "        if tag.startswith('R'): numAdverb += 1\n",
    "    features['nouns'] = numNoun\n",
    "    features['verbs'] = numVerb\n",
    "    features['adjectives'] = numAdj\n",
    "    features['adverbs'] = numAdverb\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS after only stop words remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_featuresets = [(POS_features(d, new_word_features), c) for (d, c) in phrasedocs_2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.5533"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.55325"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set_POS, test_set_POS = train_test_split(POS_featuresets, test_size=0.2, random_state=42)   #42\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set_POS)\n",
    "nltk.classify.accuracy(classifier, test_set_POS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIWC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# returns two lists:  words in positive emotion class and\n",
    "#\t\t      words in negative emotion class\n",
    "def read_words():\n",
    "  poslist = []\n",
    "  neglist = []\n",
    "\n",
    "  flexicon = open('SentimentLexicons/liwcdic2007.dic', encoding='latin1')\n",
    "  # read all LIWC words from file\n",
    "  wordlines = [line.strip() for line in flexicon]\n",
    "  # each line has a word or a stem followed by * and numbers of the word classes it is in\n",
    "  # word class 126 is positive emotion and 127 is negative emotion\n",
    "  for line in wordlines:\n",
    "    if not line == '':\n",
    "      items = line.split()\n",
    "      word = items[0]\n",
    "      classes = items[1:]\n",
    "      for c in classes:\n",
    "        if c == '126':\n",
    "          poslist.append( word )\n",
    "        if c == '127':\n",
    "          neglist.append( word )\n",
    "  return (poslist, neglist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['accept',\n",
       "  'accepta*',\n",
       "  'accepted',\n",
       "  'accepting',\n",
       "  'accepts',\n",
       "  'active*',\n",
       "  'admir*',\n",
       "  'ador*',\n",
       "  'advantag*',\n",
       "  'adventur*',\n",
       "  'affection*',\n",
       "  'agree',\n",
       "  'agreeab*',\n",
       "  'agreed',\n",
       "  'agreeing',\n",
       "  'agreement*',\n",
       "  'agrees',\n",
       "  'alright*',\n",
       "  'amaz*',\n",
       "  'amor*',\n",
       "  'amus*',\n",
       "  'aok',\n",
       "  'appreciat*',\n",
       "  'assur*',\n",
       "  'attachment*',\n",
       "  'attract*',\n",
       "  'award*',\n",
       "  'awesome',\n",
       "  'beaut*',\n",
       "  'beloved',\n",
       "  'benefic*',\n",
       "  'benefit',\n",
       "  'benefits',\n",
       "  'benefitt*',\n",
       "  'benevolen*',\n",
       "  'benign*',\n",
       "  'best',\n",
       "  'better',\n",
       "  'bless*',\n",
       "  'bold*',\n",
       "  'bonus*',\n",
       "  'brave*',\n",
       "  'bright*',\n",
       "  'brillian*',\n",
       "  'calm*',\n",
       "  'care',\n",
       "  'cared',\n",
       "  'carefree',\n",
       "  'careful*',\n",
       "  'cares',\n",
       "  'caring',\n",
       "  'casual',\n",
       "  'casually',\n",
       "  'certain*',\n",
       "  'challeng*',\n",
       "  'champ*',\n",
       "  'charit*',\n",
       "  'charm*',\n",
       "  'cheer*',\n",
       "  'cherish*',\n",
       "  'chuckl*',\n",
       "  'clever*',\n",
       "  'comed*',\n",
       "  'comfort*',\n",
       "  'commitment*',\n",
       "  'compassion*',\n",
       "  'compliment*',\n",
       "  'confidence',\n",
       "  'confident',\n",
       "  'confidently',\n",
       "  'considerate',\n",
       "  'contented*',\n",
       "  'contentment',\n",
       "  'convinc*',\n",
       "  'cool',\n",
       "  'courag*',\n",
       "  'create*',\n",
       "  'creati*',\n",
       "  'credit*',\n",
       "  'cute*',\n",
       "  'cutie*',\n",
       "  'daring',\n",
       "  'darlin*',\n",
       "  'dear*',\n",
       "  'definite',\n",
       "  'definitely',\n",
       "  'delectabl*',\n",
       "  'delicate*',\n",
       "  'delicious*',\n",
       "  'deligh*',\n",
       "  'determina*',\n",
       "  'determined',\n",
       "  'devot*',\n",
       "  'digni*',\n",
       "  'divin*',\n",
       "  'dynam*',\n",
       "  'eager*',\n",
       "  'ease*',\n",
       "  'easie*',\n",
       "  'easily',\n",
       "  'easiness',\n",
       "  'easing',\n",
       "  'easy*',\n",
       "  'ecsta*',\n",
       "  'efficien*',\n",
       "  'elegan*',\n",
       "  'encourag*',\n",
       "  'energ*',\n",
       "  'engag*',\n",
       "  'enjoy*',\n",
       "  'entertain*',\n",
       "  'enthus*',\n",
       "  'excel*',\n",
       "  'excit*',\n",
       "  'fab',\n",
       "  'fabulous*',\n",
       "  'faith*',\n",
       "  'fantastic*',\n",
       "  'favor*',\n",
       "  'favour*',\n",
       "  'fearless*',\n",
       "  'festiv*',\n",
       "  'fiesta*',\n",
       "  'fine',\n",
       "  'flatter*',\n",
       "  'flawless*',\n",
       "  'flexib*',\n",
       "  'flirt*',\n",
       "  'fond',\n",
       "  'fondly',\n",
       "  'fondness',\n",
       "  'forgave',\n",
       "  'forgiv*',\n",
       "  'free',\n",
       "  'freeb*',\n",
       "  'freed*',\n",
       "  'freeing',\n",
       "  'freely',\n",
       "  'freeness',\n",
       "  'freer',\n",
       "  'frees*',\n",
       "  'friend*',\n",
       "  'fun',\n",
       "  'funn*',\n",
       "  'genero*',\n",
       "  'gentle',\n",
       "  'gentler',\n",
       "  'gentlest',\n",
       "  'gently',\n",
       "  'giggl*',\n",
       "  'giver*',\n",
       "  'giving',\n",
       "  'glad',\n",
       "  'gladly',\n",
       "  'glamor*',\n",
       "  'glamour*',\n",
       "  'glori*',\n",
       "  'glory',\n",
       "  'good',\n",
       "  'goodness',\n",
       "  'gorgeous*',\n",
       "  'grace',\n",
       "  'graced',\n",
       "  'graceful*',\n",
       "  'graces',\n",
       "  'graci*',\n",
       "  'grand',\n",
       "  'grande*',\n",
       "  'gratef*',\n",
       "  'grati*',\n",
       "  'great',\n",
       "  'grin',\n",
       "  'grinn*',\n",
       "  'grins',\n",
       "  'ha',\n",
       "  'haha*',\n",
       "  'handsom*',\n",
       "  'happi*',\n",
       "  'happy',\n",
       "  'harmless*',\n",
       "  'harmon*',\n",
       "  'heartfelt',\n",
       "  'heartwarm*',\n",
       "  'heaven*',\n",
       "  'heh*',\n",
       "  'helper*',\n",
       "  'helpful*',\n",
       "  'helping',\n",
       "  'helps',\n",
       "  'hero*',\n",
       "  'hilarious',\n",
       "  'hoho*',\n",
       "  'honest*',\n",
       "  'honor*',\n",
       "  'honour*',\n",
       "  'hope',\n",
       "  'hoped',\n",
       "  'hopeful',\n",
       "  'hopefully',\n",
       "  'hopefulness',\n",
       "  'hopes',\n",
       "  'hoping',\n",
       "  'hug',\n",
       "  'hugg*',\n",
       "  'hugs',\n",
       "  'humor*',\n",
       "  'humour*',\n",
       "  'hurra*',\n",
       "  'ideal*',\n",
       "  'importan*',\n",
       "  'impress*',\n",
       "  'improve*',\n",
       "  'improving',\n",
       "  'incentive*',\n",
       "  'innocen*',\n",
       "  'inspir*',\n",
       "  'intell*',\n",
       "  'interest*',\n",
       "  'invigor*',\n",
       "  'joke*',\n",
       "  'joking',\n",
       "  'joll*',\n",
       "  'joy*',\n",
       "  'keen*',\n",
       "  'kidding',\n",
       "  'kindly',\n",
       "  'kindn*',\n",
       "  'kiss*',\n",
       "  'laidback',\n",
       "  'laugh*',\n",
       "  'libert*',\n",
       "  'likeab*',\n",
       "  'liked',\n",
       "  'likes',\n",
       "  'liking',\n",
       "  'livel*',\n",
       "  'lmao',\n",
       "  'lol',\n",
       "  'love',\n",
       "  'loved',\n",
       "  'lovely',\n",
       "  'lover*',\n",
       "  'loves',\n",
       "  'loving*',\n",
       "  'loyal*',\n",
       "  'luck',\n",
       "  'lucked',\n",
       "  'lucki*',\n",
       "  'lucks',\n",
       "  'lucky',\n",
       "  'madly',\n",
       "  'magnific*',\n",
       "  'merit*',\n",
       "  'merr*',\n",
       "  'neat*',\n",
       "  'nice*',\n",
       "  'nurtur*',\n",
       "  'ok',\n",
       "  'okay',\n",
       "  'okays',\n",
       "  'oks',\n",
       "  'openminded*',\n",
       "  'openness',\n",
       "  'opport*',\n",
       "  'optimal*',\n",
       "  'optimi*',\n",
       "  'original',\n",
       "  'outgoing',\n",
       "  'painl*',\n",
       "  'palatabl*',\n",
       "  'paradise',\n",
       "  'partie*',\n",
       "  'party*',\n",
       "  'passion*',\n",
       "  'peace*',\n",
       "  'perfect*',\n",
       "  'play',\n",
       "  'played',\n",
       "  'playful*',\n",
       "  'playing',\n",
       "  'plays',\n",
       "  'pleasant*',\n",
       "  'please*',\n",
       "  'pleasing',\n",
       "  'pleasur*',\n",
       "  'popular*',\n",
       "  'positiv*',\n",
       "  'prais*',\n",
       "  'precious*',\n",
       "  'prettie*',\n",
       "  'pretty',\n",
       "  'pride',\n",
       "  'privileg*',\n",
       "  'prize*',\n",
       "  'profit*',\n",
       "  'promis*',\n",
       "  'proud*',\n",
       "  'radian*',\n",
       "  'readiness',\n",
       "  'ready',\n",
       "  'reassur*',\n",
       "  'relax*',\n",
       "  'relief',\n",
       "  'reliev*',\n",
       "  'resolv*',\n",
       "  'respect',\n",
       "  'revigor*',\n",
       "  'reward*',\n",
       "  'rich*',\n",
       "  'rofl',\n",
       "  'romanc*',\n",
       "  'romantic*',\n",
       "  'safe*',\n",
       "  'satisf*',\n",
       "  'save',\n",
       "  'scrumptious*',\n",
       "  'secur*',\n",
       "  'sentimental*',\n",
       "  'share',\n",
       "  'shared',\n",
       "  'shares',\n",
       "  'sharing',\n",
       "  'silli*',\n",
       "  'silly',\n",
       "  'sincer*',\n",
       "  'smart*',\n",
       "  'smil*',\n",
       "  'sociab*',\n",
       "  'soulmate*',\n",
       "  'special',\n",
       "  'splend*',\n",
       "  'strength*',\n",
       "  'strong*',\n",
       "  'succeed*',\n",
       "  'success*',\n",
       "  'sunnier',\n",
       "  'sunniest',\n",
       "  'sunny',\n",
       "  'sunshin*',\n",
       "  'super',\n",
       "  'superior*',\n",
       "  'support',\n",
       "  'supported',\n",
       "  'supporter*',\n",
       "  'supporting',\n",
       "  'supportive*',\n",
       "  'supports',\n",
       "  'suprem*',\n",
       "  'sure*',\n",
       "  'surpris*',\n",
       "  'sweet',\n",
       "  'sweetheart*',\n",
       "  'sweetie*',\n",
       "  'sweetly',\n",
       "  'sweetness*',\n",
       "  'sweets',\n",
       "  'talent*',\n",
       "  'tehe',\n",
       "  'tender*',\n",
       "  'terrific*',\n",
       "  'thank',\n",
       "  'thanked',\n",
       "  'thankf*',\n",
       "  'thanks',\n",
       "  'thoughtful*',\n",
       "  'thrill*',\n",
       "  'toleran*',\n",
       "  'tranquil*',\n",
       "  'treasur*',\n",
       "  'treat',\n",
       "  'triumph*',\n",
       "  'true',\n",
       "  'trueness',\n",
       "  'truer',\n",
       "  'truest',\n",
       "  'truly',\n",
       "  'trust*',\n",
       "  'truth*',\n",
       "  'useful*',\n",
       "  'valuabl*',\n",
       "  'value',\n",
       "  'valued',\n",
       "  'values',\n",
       "  'valuing',\n",
       "  'vigor*',\n",
       "  'vigour*',\n",
       "  'virtue*',\n",
       "  'virtuo*',\n",
       "  'vital*',\n",
       "  'warm*',\n",
       "  'wealth*',\n",
       "  'welcom*',\n",
       "  'well',\n",
       "  'win',\n",
       "  'winn*',\n",
       "  'wins',\n",
       "  'wisdom',\n",
       "  'wise*',\n",
       "  'won',\n",
       "  'wonderf*',\n",
       "  'worship*',\n",
       "  'worthwhile',\n",
       "  'wow*',\n",
       "  'yay',\n",
       "  'yays'],\n",
       " ['abandon*',\n",
       "  'abuse*',\n",
       "  'abusi*',\n",
       "  'ache*',\n",
       "  'aching',\n",
       "  'advers*',\n",
       "  'afraid',\n",
       "  'aggravat*',\n",
       "  'aggress*',\n",
       "  'agitat*',\n",
       "  'agoniz*',\n",
       "  'agony',\n",
       "  'alarm*',\n",
       "  'alone',\n",
       "  'anger*',\n",
       "  'angr*',\n",
       "  'anguish*',\n",
       "  'annoy*',\n",
       "  'antagoni*',\n",
       "  'anxi*',\n",
       "  'apath*',\n",
       "  'appall*',\n",
       "  'apprehens*',\n",
       "  'argh*',\n",
       "  'argu*',\n",
       "  'arrogan*',\n",
       "  'asham*',\n",
       "  'assault*',\n",
       "  'asshole*',\n",
       "  'attack*',\n",
       "  'aversi*',\n",
       "  'avoid*',\n",
       "  'awful',\n",
       "  'awkward*',\n",
       "  'bad',\n",
       "  'bashful*',\n",
       "  'bastard*',\n",
       "  'battl*',\n",
       "  'beaten',\n",
       "  'bitch*',\n",
       "  'bitter*',\n",
       "  'blam*',\n",
       "  'bore*',\n",
       "  'boring',\n",
       "  'bother*',\n",
       "  'broke',\n",
       "  'brutal*',\n",
       "  'burden*',\n",
       "  'careless*',\n",
       "  'cheat*',\n",
       "  'complain*',\n",
       "  'confront*',\n",
       "  'confus*',\n",
       "  'contempt*',\n",
       "  'contradic*',\n",
       "  'crap',\n",
       "  'crappy',\n",
       "  'craz*',\n",
       "  'cried',\n",
       "  'cries',\n",
       "  'critical',\n",
       "  'critici*',\n",
       "  'crude*',\n",
       "  'cruel*',\n",
       "  'crushed',\n",
       "  'cry',\n",
       "  'crying',\n",
       "  'cunt*',\n",
       "  'cut',\n",
       "  'cynic',\n",
       "  'damag*',\n",
       "  'damn*',\n",
       "  'danger*',\n",
       "  'daze*',\n",
       "  'decay*',\n",
       "  'defeat*',\n",
       "  'defect*',\n",
       "  'defenc*',\n",
       "  'defens*',\n",
       "  'degrad*',\n",
       "  'depress*',\n",
       "  'depriv*',\n",
       "  'despair*',\n",
       "  'desperat*',\n",
       "  'despis*',\n",
       "  'destroy*',\n",
       "  'destruct*',\n",
       "  'devastat*',\n",
       "  'devil*',\n",
       "  'difficult*',\n",
       "  'disadvantage*',\n",
       "  'disagree*',\n",
       "  'disappoint*',\n",
       "  'disaster*',\n",
       "  'discomfort*',\n",
       "  'discourag*',\n",
       "  'disgust*',\n",
       "  'dishearten*',\n",
       "  'disillusion*',\n",
       "  'dislike',\n",
       "  'disliked',\n",
       "  'dislikes',\n",
       "  'disliking',\n",
       "  'dismay*',\n",
       "  'dissatisf*',\n",
       "  'distract*',\n",
       "  'distraught',\n",
       "  'distress*',\n",
       "  'distrust*',\n",
       "  'disturb*',\n",
       "  'domina*',\n",
       "  'doom*',\n",
       "  'dork*',\n",
       "  'doubt*',\n",
       "  'dread*',\n",
       "  'dull*',\n",
       "  'dumb*',\n",
       "  'dump*',\n",
       "  'dwell*',\n",
       "  'egotis*',\n",
       "  'embarrass*',\n",
       "  'emotional',\n",
       "  'empt*',\n",
       "  'enemie*',\n",
       "  'enemy*',\n",
       "  'enrag*',\n",
       "  'envie*',\n",
       "  'envious',\n",
       "  'envy*',\n",
       "  'evil*',\n",
       "  'excruciat*',\n",
       "  'exhaust*',\n",
       "  'fail*',\n",
       "  'fake',\n",
       "  'fatal*',\n",
       "  'fatigu*',\n",
       "  'fault*',\n",
       "  'fear',\n",
       "  'feared',\n",
       "  'fearful*',\n",
       "  'fearing',\n",
       "  'fears',\n",
       "  'feroc*',\n",
       "  'feud*',\n",
       "  'fiery',\n",
       "  'fight*',\n",
       "  'fired',\n",
       "  'flunk*',\n",
       "  'foe*',\n",
       "  'fool*',\n",
       "  'forbid*',\n",
       "  'fought',\n",
       "  'frantic*',\n",
       "  'freak*',\n",
       "  'fright*',\n",
       "  'frustrat*',\n",
       "  'fuck',\n",
       "  'fucked*',\n",
       "  'fucker*',\n",
       "  'fuckin*',\n",
       "  'fucks',\n",
       "  'fume*',\n",
       "  'fuming',\n",
       "  'furious*',\n",
       "  'fury',\n",
       "  'geek*',\n",
       "  'gloom*',\n",
       "  'goddam*',\n",
       "  'gossip*',\n",
       "  'grave*',\n",
       "  'greed*',\n",
       "  'grief',\n",
       "  'griev*',\n",
       "  'grim*',\n",
       "  'gross*',\n",
       "  'grouch*',\n",
       "  'grr*',\n",
       "  'guilt*',\n",
       "  'harass*',\n",
       "  'harm',\n",
       "  'harmed',\n",
       "  'harmful*',\n",
       "  'harming',\n",
       "  'harms',\n",
       "  'hate',\n",
       "  'hated',\n",
       "  'hateful*',\n",
       "  'hater*',\n",
       "  'hates',\n",
       "  'hating',\n",
       "  'hatred',\n",
       "  'heartbreak*',\n",
       "  'heartbroke*',\n",
       "  'heartless*',\n",
       "  'hell',\n",
       "  'hellish',\n",
       "  'helpless*',\n",
       "  'hesita*',\n",
       "  'homesick*',\n",
       "  'hopeless*',\n",
       "  'horr*',\n",
       "  'hostil*',\n",
       "  'humiliat*',\n",
       "  'hurt*',\n",
       "  'idiot*',\n",
       "  'ignor*',\n",
       "  'immoral*',\n",
       "  'impatien*',\n",
       "  'impersonal',\n",
       "  'impolite*',\n",
       "  'inadequa*',\n",
       "  'indecis*',\n",
       "  'ineffect*',\n",
       "  'inferior*',\n",
       "  'inhib*',\n",
       "  'insecur*',\n",
       "  'insincer*',\n",
       "  'insult*',\n",
       "  'interrup*',\n",
       "  'intimidat*',\n",
       "  'irrational*',\n",
       "  'irrita*',\n",
       "  'isolat*',\n",
       "  'jaded',\n",
       "  'jealous*',\n",
       "  'jerk',\n",
       "  'jerked',\n",
       "  'jerks',\n",
       "  'kill*',\n",
       "  'lame*',\n",
       "  'lazie*',\n",
       "  'lazy',\n",
       "  'liabilit*',\n",
       "  'liar*',\n",
       "  'lied',\n",
       "  'lies',\n",
       "  'lone*',\n",
       "  'longing*',\n",
       "  'lose',\n",
       "  'loser*',\n",
       "  'loses',\n",
       "  'losing',\n",
       "  'loss*',\n",
       "  'lost',\n",
       "  'lous*',\n",
       "  'low*',\n",
       "  'luckless*',\n",
       "  'ludicrous*',\n",
       "  'lying',\n",
       "  'mad',\n",
       "  'maddening',\n",
       "  'madder',\n",
       "  'maddest',\n",
       "  'maniac*',\n",
       "  'masochis*',\n",
       "  'melanchol*',\n",
       "  'mess',\n",
       "  'messy',\n",
       "  'miser*',\n",
       "  'miss',\n",
       "  'missed',\n",
       "  'misses',\n",
       "  'missing',\n",
       "  'mistak*',\n",
       "  'mock',\n",
       "  'mocked',\n",
       "  'mocker*',\n",
       "  'mocking',\n",
       "  'mocks',\n",
       "  'molest*',\n",
       "  'mooch*',\n",
       "  'moodi*',\n",
       "  'moody',\n",
       "  'moron*',\n",
       "  'mourn*',\n",
       "  'murder*',\n",
       "  'nag*',\n",
       "  'nast*',\n",
       "  'needy',\n",
       "  'neglect*',\n",
       "  'nerd*',\n",
       "  'nervous*',\n",
       "  'neurotic*',\n",
       "  'numb*',\n",
       "  'obnoxious*',\n",
       "  'obsess*',\n",
       "  'offence*',\n",
       "  'offend*',\n",
       "  'offens*',\n",
       "  'outrag*',\n",
       "  'overwhelm*',\n",
       "  'pain',\n",
       "  'pained',\n",
       "  'painf*',\n",
       "  'paining',\n",
       "  'pains',\n",
       "  'panic*',\n",
       "  'paranoi*',\n",
       "  'pathetic*',\n",
       "  'peculiar*',\n",
       "  'perver*',\n",
       "  'pessimis*',\n",
       "  'petrif*',\n",
       "  'pettie*',\n",
       "  'petty*',\n",
       "  'phobi*',\n",
       "  'piss*',\n",
       "  'piti*',\n",
       "  'pity*',\n",
       "  'poison*',\n",
       "  'prejudic*',\n",
       "  'pressur*',\n",
       "  'prick*',\n",
       "  'problem*',\n",
       "  'protest',\n",
       "  'protested',\n",
       "  'protesting',\n",
       "  'puk*',\n",
       "  'punish*',\n",
       "  'rage*',\n",
       "  'raging',\n",
       "  'rancid*',\n",
       "  'rape*',\n",
       "  'raping',\n",
       "  'rapist*',\n",
       "  'rebel*',\n",
       "  'reek*',\n",
       "  'regret*',\n",
       "  'reject*',\n",
       "  'reluctan*',\n",
       "  'remorse*',\n",
       "  'repress*',\n",
       "  'resent*',\n",
       "  'resign*',\n",
       "  'restless*',\n",
       "  'revenge*',\n",
       "  'ridicul*',\n",
       "  'rigid*',\n",
       "  'risk*',\n",
       "  'rotten',\n",
       "  'rude*',\n",
       "  'ruin*',\n",
       "  'sad',\n",
       "  'sadde*',\n",
       "  'sadly',\n",
       "  'sadness',\n",
       "  'sarcas*',\n",
       "  'savage*',\n",
       "  'scare*',\n",
       "  'scaring',\n",
       "  'scary',\n",
       "  'sceptic*',\n",
       "  'scream*',\n",
       "  'screw*',\n",
       "  'selfish*',\n",
       "  'serious',\n",
       "  'seriously',\n",
       "  'seriousness',\n",
       "  'severe*',\n",
       "  'shake*',\n",
       "  'shaki*',\n",
       "  'shaky',\n",
       "  'shame*',\n",
       "  'shit*',\n",
       "  'shock*',\n",
       "  'shook',\n",
       "  'shy*',\n",
       "  'sicken*',\n",
       "  'sin',\n",
       "  'sinister',\n",
       "  'sins',\n",
       "  'skeptic*',\n",
       "  'slut*',\n",
       "  'smother*',\n",
       "  'smug*',\n",
       "  'snob*',\n",
       "  'sob',\n",
       "  'sobbed',\n",
       "  'sobbing',\n",
       "  'sobs',\n",
       "  'solemn*',\n",
       "  'sorrow*',\n",
       "  'sorry',\n",
       "  'spite*',\n",
       "  'stammer*',\n",
       "  'stank',\n",
       "  'startl*',\n",
       "  'steal*',\n",
       "  'stench*',\n",
       "  'stink*',\n",
       "  'strain*',\n",
       "  'strange',\n",
       "  'stress*',\n",
       "  'struggl*',\n",
       "  'stubborn*',\n",
       "  'stunk',\n",
       "  'stunned',\n",
       "  'stuns',\n",
       "  'stupid*',\n",
       "  'stutter*',\n",
       "  'submissive*',\n",
       "  'suck',\n",
       "  'sucked',\n",
       "  'sucker*',\n",
       "  'sucks',\n",
       "  'sucky',\n",
       "  'suffer',\n",
       "  'suffered',\n",
       "  'sufferer*',\n",
       "  'suffering',\n",
       "  'suffers',\n",
       "  'suspicio*',\n",
       "  'tantrum*',\n",
       "  'tears',\n",
       "  'teas*',\n",
       "  'temper',\n",
       "  'tempers',\n",
       "  'tense*',\n",
       "  'tensing',\n",
       "  'tension*',\n",
       "  'terribl*',\n",
       "  'terrified',\n",
       "  'terrifies',\n",
       "  'terrify',\n",
       "  'terrifying',\n",
       "  'terror*',\n",
       "  'thief',\n",
       "  'thieve*',\n",
       "  'threat*',\n",
       "  'ticked',\n",
       "  'timid*',\n",
       "  'tortur*',\n",
       "  'tough*',\n",
       "  'traged*',\n",
       "  'tragic*',\n",
       "  'trauma*',\n",
       "  'trembl*',\n",
       "  'trick*',\n",
       "  'trite',\n",
       "  'trivi*',\n",
       "  'troubl*',\n",
       "  'turmoil',\n",
       "  'ugh',\n",
       "  'ugl*',\n",
       "  'unattractive',\n",
       "  'uncertain*',\n",
       "  'uncomfortabl*',\n",
       "  'uncontrol*',\n",
       "  'uneas*',\n",
       "  'unfortunate*',\n",
       "  'unfriendly',\n",
       "  'ungrateful*',\n",
       "  'unhapp*',\n",
       "  'unimportant',\n",
       "  'unimpress*',\n",
       "  'unkind',\n",
       "  'unlov*',\n",
       "  'unpleasant',\n",
       "  'unprotected',\n",
       "  'unsavo*',\n",
       "  'unsuccessful*',\n",
       "  'unsure*',\n",
       "  'unwelcom*',\n",
       "  'upset*',\n",
       "  'uptight*',\n",
       "  'useless*',\n",
       "  'vain',\n",
       "  'vanity',\n",
       "  'vicious*',\n",
       "  'victim*',\n",
       "  'vile',\n",
       "  'villain*',\n",
       "  'violat*',\n",
       "  'violent*',\n",
       "  'vulnerab*',\n",
       "  'vulture*',\n",
       "  'war',\n",
       "  'warfare*',\n",
       "  'warred',\n",
       "  'warring',\n",
       "  'wars',\n",
       "  'weak*',\n",
       "  'weapon*',\n",
       "  'weep*',\n",
       "  'weird*',\n",
       "  'wept',\n",
       "  'whine*',\n",
       "  'whining',\n",
       "  'whore*',\n",
       "  'wicked*',\n",
       "  'wimp*',\n",
       "  'witch',\n",
       "  'woe*',\n",
       "  'worr*',\n",
       "  'worse*',\n",
       "  'worst',\n",
       "  'worthless*',\n",
       "  'wrong*',\n",
       "  'yearn*'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "(poslist, neglist) = read_words()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isPresent(word, emotionlist):\n",
    "  isFound = False\n",
    "  # loop over all elements of list\n",
    "  for emotionword in emotionlist:\n",
    "    # test if a word or a stem\n",
    "    if not emotionword[-1] == '*':\n",
    "      # it's a word!\n",
    "      # when a match is found, can quit the loop with True\n",
    "      if word == emotionword:\n",
    "        isFound = True\n",
    "        break\n",
    "    else:\n",
    "      # it's a stem!\n",
    "      # when a match is found, can quit the loop with True\n",
    "      if word.startswith(emotionword[0:-1]):\n",
    "        isFound = True\n",
    "        break\n",
    "  # end of loop\n",
    "  return isFound\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LIWC_features(document, poslist, neglist):\n",
    "    features = {}\n",
    "    positive_count = sum(1 for word in document if isPresent(word, poslist))\n",
    "    negative_count = sum(1 for word in document if isPresent(word, neglist))\n",
    "\n",
    "    features[\"positive_count\"] = positive_count\n",
    "    features[\"negative_count\"] = negative_count\n",
    "    features[\"sentiment_strength\"] = positive_count - negative_count  \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'phrasedocs_2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m LIWC_featuresets \u001b[38;5;241m=\u001b[39m [(LIWC_features(d, poslist, neglist), c) \u001b[38;5;28;01mfor\u001b[39;00m (d, c) \u001b[38;5;129;01min\u001b[39;00m \u001b[43mphrasedocs_2\u001b[49m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'phrasedocs_2' is not defined"
     ]
    }
   ],
   "source": [
    "LIWC_featuresets = [(LIWC_features(d, poslist, neglist), c) for (d, c) in phrasedocs_2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.534"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.53"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set_LIWC, test_set_LIWC = train_test_split(LIWC_featuresets, test_size=0.2, random_state=42)   #42\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set_LIWC)\n",
    "nltk.classify.accuracy(classifier, test_set_LIWC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine bigram and LIWC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_features(document, word_features, bigram_features, poslist, neglist):\n",
    "    \n",
    "    bigram_feats = bigram_document_features(document, word_features, bigram_features)\n",
    "    \n",
    "    \n",
    "    liwc_features = LIWC_features(document, poslist, neglist)\n",
    "    \n",
    "    \n",
    "    features = {}\n",
    "    features.update(bigram_feats)  \n",
    "    features.update(liwc_features)  \n",
    "    \n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_featuresets = [\n",
    "    (combined_features(d, new_word_features, bigram_features, poslist, neglist), c)\n",
    "    for (d, c) in phrasedocs_2\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.56325 combined with Bigram features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.56325"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set_combine, test_set_combine = train_test_split(combined_featuresets, test_size=0.2, random_state=42)   #42\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set_combine)\n",
    "\n",
    "nltk.classify.accuracy(classifier, test_set_combine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each fold size: 6666\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[88], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mcross_validation_PRF\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_folds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcombined_featuresets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[74], line 15\u001b[0m, in \u001b[0;36mcross_validation_PRF\u001b[1;34m(num_folds, featuresets, labels)\u001b[0m\n\u001b[0;32m     13\u001b[0m train_this_round \u001b[38;5;241m=\u001b[39m featuresets[:(i\u001b[38;5;241m*\u001b[39msubset_size)] \u001b[38;5;241m+\u001b[39m featuresets[((i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39msubset_size):]\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# train using train_this_round\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m classifier \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mNaiveBayesClassifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_this_round\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# evaluate against test_this_round to produce the gold and predicted labels\u001b[39;00m\n\u001b[0;32m     17\u001b[0m goldlist \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\nltk\\classify\\naivebayes.py:218\u001b[0m, in \u001b[0;36mNaiveBayesClassifier.train\u001b[1;34m(cls, labeled_featuresets, estimator)\u001b[0m\n\u001b[0;32m    216\u001b[0m         feature_values[fname]\u001b[38;5;241m.\u001b[39madd(fval)\n\u001b[0;32m    217\u001b[0m         \u001b[38;5;66;03m# Keep a list of all feature names.\u001b[39;00m\n\u001b[1;32m--> 218\u001b[0m         \u001b[43mfnames\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;66;03m# If a feature didn't have a value given for an instance, then\u001b[39;00m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;66;03m# we assume that it gets the implicit value 'None.'  This loop\u001b[39;00m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;66;03m# counts up the number of 'missing' feature values for each\u001b[39;00m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;66;03m# (label,fname) pair, and increments the count of the fval\u001b[39;00m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;66;03m# 'None' by that amount.\u001b[39;00m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m label_freqdist:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cross_validation_PRF(num_folds, combined_featuresets, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Phrase'] = test['Phrase'].fillna('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "english_stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Phrase'] = [re.sub(r'\\d+', 'num', re.sub(r\"[^\\s\\w]\", \"\", t)).lower() for t in train.Phrase]\n",
    "test['Phrase'] =  [re.sub(r'\\d+', 'num', re.sub(r\"[^\\s\\w]\", \"\", t)).lower() for t in test.Phrase]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer =lambda text:[stemmer.stem(token) for token in word_tokenize(text) if token not in newstopwords],\n",
    "                            ngram_range=(1,2),\n",
    "                            max_features=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\simba\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\feature_extraction\\text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['abandon', 'abil', 'abl', 'absolut', 'absorb', 'abstract',\n",
       "       'absurd', 'abus', 'accent', 'accept', 'access', 'accomplish',\n",
       "       'accur', 'ach', 'achiev', 'across', 'act', 'action', 'action film',\n",
       "       'action movi', 'action sequenc', 'actor', 'actress', 'actual',\n",
       "       'ad', 'adam', 'adam sandler', 'adapt', 'add', 'addit', 'adequ',\n",
       "       'admir', 'admit', 'adolesc', 'adult', 'adventur', 'affair',\n",
       "       'affect', 'afraid', 'afterschool', 'age', 'ago', 'ahead', 'aim',\n",
       "       'aimless', 'air', 'alien', 'aliv', 'allen', 'allow', 'almost',\n",
       "       'alon', 'along', 'alreadi', 'also', 'altern', 'although', 'alway',\n",
       "       'amateurish', 'amaz', 'ambigu', 'ambit', 'ambiti', 'america',\n",
       "       'american', 'among', 'amount', 'amus', 'analyz', 'angel', 'angst',\n",
       "       'anim', 'ann', 'annoy', 'anoth', 'answer', 'anyon', 'anyth',\n",
       "       'apart', 'appar', 'appeal', 'appear', 'appli', 'appreci',\n",
       "       'approach', 'appropri', 'area', 'argu', 'argument', 'around',\n",
       "       'arriv', 'art', 'artifici', 'artist', 'ask', 'aspect', 'aspir',\n",
       "       'assassin', 'associ', 'assur', 'astonish', 'atmospher', 'attempt',\n",
       "       'attent', 'attract', 'audienc', 'authent', 'author', 'averag',\n",
       "       'avoid', 'aw', 'award', 'away', 'awkward', 'b', 'back',\n",
       "       'background', 'bad', 'bad movi', 'baffl', 'bag', 'balanc', 'banal',\n",
       "       'band', 'bare', 'barrel', 'barri', 'base', 'basic', 'battl',\n",
       "       'bear', 'beast', 'beat', 'beauti', 'becom', 'begin', 'behavior',\n",
       "       'behind', 'believ', 'belli', 'belt', 'beneath', 'benefit',\n",
       "       'benigni', 'best', 'best film', 'better', 'beyond', 'big',\n",
       "       'big screen'], dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.fit(train.Phrase)\n",
    "vectorizer.get_feature_names_out()[:150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = vectorizer.transform(train.Phrase)\n",
    "y = train.Sentiment\n",
    "X_test = vectorizer.transform(test.Phrase)\n",
    "\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
       "\twith 369497 stored elements and shape (124848, 2000)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.630718954248366"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred1 = model.predict(X_val)\n",
    "\n",
    "accuracy_score(y_val, y_pred1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_PRF(num_folds, featuresets, labels):\n",
    "    subset_size = int(len(featuresets)/num_folds)\n",
    "    print('Each fold size:', subset_size)\n",
    "    # for the number of labels - start the totals lists with zeroes\n",
    "    num_labels = len(labels)\n",
    "    total_precision_list = [0] * num_labels\n",
    "    total_recall_list = [0] * num_labels\n",
    "    total_F1_list = [0] * num_labels\n",
    "\n",
    "    # iterate over the folds\n",
    "    for i in range(num_folds):\n",
    "        test_this_round = featuresets[(i*subset_size):][:subset_size]\n",
    "        train_this_round = featuresets[:(i*subset_size)] + featuresets[((i+1)*subset_size):]\n",
    "        # train using train_this_round\n",
    "        classifier = nltk.NaiveBayesClassifier.train(train_this_round)\n",
    "        # evaluate against test_this_round to produce the gold and predicted labels\n",
    "        goldlist = []\n",
    "        predictedlist = []\n",
    "        for (features, label) in test_this_round:\n",
    "            goldlist.append(label)\n",
    "            predictedlist.append(classifier.classify(features))\n",
    "\n",
    "        # computes evaluation measures for this fold and\n",
    "        #   returns list of measures for each label\n",
    "        print('Fold', i)\n",
    "        (precision_list, recall_list, F1_list) \\\n",
    "                  = eval_measures(goldlist, predictedlist, labels)\n",
    "        # take off triple string to print precision, recall and F1 for each fold\n",
    "        '''\n",
    "        print('\\tPrecision\\tRecall\\t\\tF1')\n",
    "        # print measures for each label\n",
    "        for i, lab in enumerate(labels):\n",
    "            print(lab, '\\t', \"{:10.3f}\".format(precision_list[i]), \\\n",
    "              \"{:10.3f}\".format(recall_list[i]), \"{:10.3f}\".format(F1_list[i]))\n",
    "        '''\n",
    "        # for each label add to the sums in the total lists\n",
    "        for i in range(num_labels):\n",
    "            # for each label, add the 3 measures to the 3 lists of totals\n",
    "            total_precision_list[i] += precision_list[i]\n",
    "            total_recall_list[i] += recall_list[i]\n",
    "            total_F1_list[i] += F1_list[i]\n",
    "\n",
    "    # find precision, recall and F measure averaged over all rounds for all labels\n",
    "    # compute averages from the totals lists\n",
    "    precision_list = [tot/num_folds for tot in total_precision_list]\n",
    "    recall_list = [tot/num_folds for tot in total_recall_list]\n",
    "    F1_list = [tot/num_folds for tot in total_F1_list]\n",
    "    # the evaluation measures in a table with one row per label\n",
    "    print('\\nAverage Precision\\tRecall\\t\\tF1 \\tPer Label')\n",
    "    # print measures for each label\n",
    "    for i, lab in enumerate(labels):\n",
    "        print(lab, '\\t', \"{:10.3f}\".format(precision_list[i]), \\\n",
    "          \"{:10.3f}\".format(recall_list[i]), \"{:10.3f}\".format(F1_list[i]))\n",
    "    \n",
    "    # print macro average over all labels - treats each label equally\n",
    "    print('\\nMacro Average Precision\\tRecall\\t\\tF1 \\tOver All Labels')\n",
    "    print('\\t', \"{:10.3f}\".format(sum(precision_list)/num_labels), \\\n",
    "          \"{:10.3f}\".format(sum(recall_list)/num_labels), \\\n",
    "          \"{:10.3f}\".format(sum(F1_list)/num_labels))\n",
    "\n",
    "    # for micro averaging, weight the scores for each label by the number of items\n",
    "    #    this is better for labels with imbalance\n",
    "    # first intialize a dictionary for label counts and then count them\n",
    "    label_counts = {}\n",
    "    for lab in labels:\n",
    "      label_counts[lab] = 0 \n",
    "    # count the labels\n",
    "    for (doc, lab) in featuresets:\n",
    "      label_counts[lab] += 1\n",
    "    # make weights compared to the number of documents in featuresets\n",
    "    num_docs = len(featuresets)\n",
    "    label_weights = [(label_counts[lab] / num_docs) for lab in labels]\n",
    "    print('\\nLabel Counts', label_counts)\n",
    "    #print('Label weights', label_weights)\n",
    "    # print macro average over all labels\n",
    "    print('Micro Average Precision\\tRecall\\t\\tF1 \\tOver All Labels')\n",
    "    precision = sum([a * b for a,b in zip(precision_list, label_weights)])\n",
    "    recall = sum([a * b for a,b in zip(recall_list, label_weights)])\n",
    "    F1 = sum([a * b for a,b in zip(F1_list, label_weights)])\n",
    "    print( '\\t', \"{:10.3f}\".format(precision), \\\n",
    "      \"{:10.3f}\".format(recall), \"{:10.3f}\".format(F1))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_measures(gold, predicted, labels):\n",
    "    \n",
    "    # these lists have values for each label \n",
    "    recall_list = []\n",
    "    precision_list = []\n",
    "    F1_list = []\n",
    "\n",
    "    for lab in labels:\n",
    "        # for each label, compare gold and predicted lists and compute values\n",
    "        TP = FP = FN = TN = 0\n",
    "        for i, val in enumerate(gold):\n",
    "            if val == lab and predicted[i] == lab:  TP += 1\n",
    "            if val == lab and predicted[i] != lab:  FN += 1\n",
    "            if val != lab and predicted[i] == lab:  FP += 1\n",
    "            if val != lab and predicted[i] != lab:  TN += 1\n",
    "        # use these to compute recall, precision, F1\n",
    "        # for small numbers, guard against dividing by zero in computing measures\n",
    "        if (TP == 0) or (FP == 0) or (FN == 0):\n",
    "          recall_list.append (0)\n",
    "          precision_list.append (0)\n",
    "          F1_list.append(0)\n",
    "        else:\n",
    "          recall = TP / (TP + FP)\n",
    "          precision = TP / (TP + FN)\n",
    "          recall_list.append(recall)\n",
    "          precision_list.append(precision)\n",
    "          F1_list.append( 2 * (recall * precision) / (recall + precision))\n",
    "\n",
    "    # the evaluation measures in a table with one row per label\n",
    "    return (precision_list, recall_list, F1_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
